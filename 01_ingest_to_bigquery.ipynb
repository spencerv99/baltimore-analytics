{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baltimore Analytics — Data Ingestion Pipeline v2\n",
    "**Project:** `baltimore-analytics`  \n",
    "**Dataset:** `raw_data`  \n",
    "**API:** ArcGIS REST (replaces Socrata SODA — portal migrated post-2021)  \n",
    "**Author:** Spencer  \n",
    "\n",
    "---\n",
    "### Architecture\n",
    "```\n",
    "baltimore-analytics (GCP Project)\n",
    "├── raw_data          ← partitioned source tables (this notebook)\n",
    "├── analytics         ← cleaned, enriched, joined tables (future)\n",
    "└── views             ← Looker Studio-facing pre/post 2021 views (future)\n",
    "```\n",
    "### Pre/Post 2021 Design\n",
    "- **Crime:** Baltimore split this for us — Legacy SRS (through 12/31/2024) vs current NIBRS (2021+)\n",
    "- **311:** Consolidated FeatureServer covers 2021+; annual slugs cover pre-2021\n",
    "- **All others:** Date-partitioned; `_period` column (`pre_2021` / `post_2021`) added at ingestion\n",
    "- **Views:** Auto-created in `views` dataset at end of this notebook\n",
    "\n",
    "### Partition Strategy\n",
    "All date-partitioned tables use **MONTH** partitioning (not DAY) to stay under BigQuery's 4,000 partition limit.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run once\n",
    "# %pip install requests google-cloud-bigquery google-cloud-bigquery-storage pandas pyarrow db-dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Config loaded. 9 datasets registered.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s — %(levelname)s — %(message)s')\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# PROJECT CONFIG\n",
    "# ─────────────────────────────────────────────\n",
    "GCP_PROJECT      = \"baltimore-analytics\"           # GCP Project ID\n",
    "BQ_DATASET       = \"raw_data\"                      # BigQuery dataset\n",
    "GCP_REGION       = \"us-east1\"                      # Closest region to Baltimore\n",
    "CREDENTIALS_PATH = \"service_account.json\"          # Update this path\n",
    "\n",
    "# ArcGIS standard query parameters\n",
    "BASE_PARAMS = {\n",
    "    \"where\":          \"1=1\",\n",
    "    \"outFields\":      \"*\",\n",
    "    \"f\":              \"json\",\n",
    "    \"returnGeometry\": \"true\",\n",
    "    \"outSR\":          \"4326\",  # WGS84 — standard for BigQuery GIS\n",
    "}\n",
    "\n",
    "PAGE_SIZE = 1000  # Conservative; 311 Yearly service MaxRecordCount is 2000\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 311 LAYER INDEX\n",
    "# Single FeatureServer hosts all years as numbered layers.\n",
    "# Layer 0 = 2022, Layer 1 = 2021, ..., Layer 18 = 2004\n",
    "# 2023-present consolidated endpoint is currently unavailable.\n",
    "# ─────────────────────────────────────────────\n",
    "CSR_311_BASE = \"https://services1.arcgis.com/UWYHeuuJISiGmgXx/ArcGIS/rest/services/311_Customer_Service_Requests_Yearly/FeatureServer\"\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# DATASET REGISTRY\n",
    "# url:            Primary ArcGIS FeatureServer query endpoint\n",
    "# date_col:       Used for BigQuery partitioning and _period flag\n",
    "# partition_type: MONTH for long historical datasets (avoids 4000 partition limit)\n",
    "# union_with:     Additional endpoints to union into the same table\n",
    "# ─────────────────────────────────────────────\n",
    "DATASETS = {\n",
    "\n",
    "    # ── PUBLIC SAFETY ──────────────────────────────────────────────────────\n",
    "    \"crime_incidents_legacy\": {\n",
    "        \"url\": \"https://services1.arcgis.com/UWYHeuuJISiGmgXx/arcgis/rest/services/Part1_Crime_Beta/FeatureServer/0/query\",\n",
    "        \"description\": \"BPD Part 1 Crime — Legacy SRS through 12/31/2024. Pre-2021 analytical anchor.\",\n",
    "        \"date_col\": \"CrimeDateTime\",\n",
    "        \"partition_type\": \"MONTH\",\n",
    "        \"union_with\": [],\n",
    "        \"quality_note\": \"SRS hierarchy rule — only most serious offense per incident. Primary source for pre-2021 analysis.\"\n",
    "    },\n",
    "    \"crime_incidents_current\": {\n",
    "        \"url\": \"https://services1.arcgis.com/UWYHeuuJISiGmgXx/arcgis/rest/services/NIBRS_GroupA_Crime_Data/FeatureServer/0/query\",\n",
    "        \"description\": \"BPD NIBRS Group A Crime — current post-2021 dataset. Updated weekly.\",\n",
    "        \"date_col\": \"CrimeDateTime\",\n",
    "        \"partition_type\": \"MONTH\",\n",
    "        \"union_with\": [],\n",
    "        \"quality_note\": \"NIBRS reports all offenses per incident — not directly comparable to Legacy SRS counts.\"\n",
    "    },\n",
    "    \"bpd_arrests\": {\n",
    "        \"url\": \"https://egis.baltimorecity.gov/egis/rest/services/GeoSpatialized_Tables/Arrest/FeatureServer/0/query\",\n",
    "        \"description\": \"BPD Arrests. Updated weekly.\",\n",
    "        \"date_col\": \"ArrestDateTime\",   # Note: actual column is arrestdatetime (no separate arrestdate)\n",
    "        \"partition_type\": \"MONTH\",\n",
    "        \"union_with\": [],\n",
    "        \"quality_note\": \"~41% of records missing geo coordinates (older records never geocoded).\"\n",
    "    },\n",
    "\n",
    "    # ── HOUSING / VACANCY ──────────────────────────────────────────────────\n",
    "    \"vacant_building_notices\": {\n",
    "        \"url\": \"https://egisdata.baltimorecity.gov/egis/rest/services/Housing/DHCD_Open_Baltimore_Datasets/FeatureServer/1/query\",\n",
    "        \"description\": \"Active vacant building notices citywide. Updated daily.\",\n",
    "        \"date_col\": \"DateNotice\",\n",
    "        \"partition_type\": \"MONTH\",\n",
    "        \"union_with\": [],\n",
    "        \"quality_note\": None\n",
    "    },\n",
    "    \"vacant_building_rehabs\": {\n",
    "        \"url\": \"https://egisdata.baltimorecity.gov/egis/rest/services/Housing/DHCD_Open_Baltimore_Datasets/FeatureServer/2/query\",\n",
    "        \"description\": \"Rehab permits issued for vacant buildings. Includes HousingMarketTypology.\",\n",
    "        \"date_col\": \"DateIssued\",\n",
    "        \"union_with\": [],\n",
    "        \"quality_note\": None\n",
    "    },\n",
    "\n",
    "    # ── PERMITS / INVESTMENT ───────────────────────────────────────────────\n",
    "    \"building_permits\": {\n",
    "        \"url\": \"https://services1.arcgis.com/UWYHeuuJISiGmgXx/arcgis/rest/services/Housing_and_Building_Permits__2015_to_2018/FeatureServer/0/query\",\n",
    "        \"description\": \"Building permits 2015-present. Union of historical and current endpoints.\",\n",
    "        \"date_col\": \"DateIssued\",       # Actual column normalizes to issueddate\n",
    "        \"partition_type\": \"MONTH\",\n",
    "        \"union_with\": [\n",
    "            \"https://egisdata.baltimorecity.gov/egis/rest/services/Housing/DHCD_Open_Baltimore_Datasets/FeatureServer/3/query\"\n",
    "        ],\n",
    "        \"quality_note\": None\n",
    "    },\n",
    "\n",
    "    # ── 311 SERVICE REQUESTS ───────────────────────────────────────────────\n",
    "    # 2023-2026 consolidated endpoint unavailable — coverage is 2004-2022 only.\n",
    "    \"service_requests_311\": {\n",
    "        \"url\": f\"{CSR_311_BASE}/0/query\",\n",
    "        \"description\": \"311 CSR 2004-2022 from yearly FeatureServer layers.\",\n",
    "        \"date_col\": \"CreatedDate\",\n",
    "        \"partition_type\": \"MONTH\",\n",
    "        \"union_with\": [f\"{CSR_311_BASE}/{i}/query\" for i in range(1, 19)],\n",
    "        \"quality_note\": \"2023-2026 data excluded — consolidated endpoint returning query error. Coverage is 2004-2022.\"\n",
    "    },\n",
    "\n",
    "    # ── PROPERTY ───────────────────────────────────────────────────────────\n",
    "    \"real_property\": {\n",
    "        \"url\": \"https://geodata.baltimorecity.gov/egis/rest/services/CityView/Realproperty_OB/FeatureServer/0/query\",\n",
    "        \"description\": \"Real property — 200k+ parcels, ownership, assessed value. Full refresh (no date partition).\",\n",
    "        \"date_col\": None,\n",
    "        \"union_with\": [],\n",
    "        \"quality_note\": None\n",
    "    },\n",
    "\n",
    "    # ── REFERENCE / SPATIAL ────────────────────────────────────────────────\n",
    "    \"neighborhood_boundaries\": {\n",
    "        \"url\": \"https://geodata.baltimorecity.gov/egis/rest/services/CityView/Neighborhoods/FeatureServer/0/query\",\n",
    "        \"description\": \"Baltimore NSA polygon boundaries. Primary spatial join key. Polygon geometry captured as geo_polygon_wkt.\",\n",
    "        \"date_col\": None,\n",
    "        \"union_with\": [],\n",
    "        \"quality_note\": None\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"✓ Config loaded. {len(DATASETS)} datasets registered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 16:43:25,365 — INFO — ✓ Dataset 'baltimore-analytics.raw_data' ready.\n",
      "2026-02-26 16:43:25,563 — INFO — ✓ Dataset 'baltimore-analytics.views' ready.\n"
     ]
    }
   ],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    CREDENTIALS_PATH,\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    ")\n",
    "bq = bigquery.Client(project=GCP_PROJECT, credentials=credentials)\n",
    "\n",
    "for dataset_name in [BQ_DATASET, \"views\"]:\n",
    "    ds = bigquery.Dataset(f\"{GCP_PROJECT}.{dataset_name}\")\n",
    "    ds.location = GCP_REGION\n",
    "    bq.create_dataset(ds, exists_ok=True)\n",
    "    log.info(f\"✓ Dataset '{GCP_PROJECT}.{dataset_name}' ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import pyarrow as pa\n",
    "\n",
    "\n",
    "def fetch_arcgis_layer(url: str, page_size: int = PAGE_SIZE) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Paginate through an ArcGIS FeatureServer layer.\n",
    "    Handles both point geometry (x/y) and polygon geometry (rings).\n",
    "    Includes retry logic with exponential backoff for rate limiting.\n",
    "    \"\"\"\n",
    "    all_records = []\n",
    "    offset = 0\n",
    "    max_retries = 5\n",
    "\n",
    "    try:\n",
    "        count_resp = requests.get(\n",
    "            url, params={\"where\": \"1=1\", \"returnCountOnly\": \"true\", \"f\": \"json\"}, timeout=15\n",
    "        )\n",
    "        total = count_resp.json().get(\"count\", \"unknown\")\n",
    "        log.info(f\"  Total records: {total}\")\n",
    "    except Exception:\n",
    "        log.info(\"  Could not get total count — proceeding with pagination.\")\n",
    "\n",
    "    while True:\n",
    "        params = {**BASE_PARAMS, \"resultOffset\": offset, \"resultRecordCount\": page_size}\n",
    "\n",
    "        # Retry loop with exponential backoff\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                resp = requests.get(url, params=params, timeout=60)\n",
    "                resp.raise_for_status()\n",
    "                data = resp.json()\n",
    "                break  # success\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait = 2 ** attempt * 3  # 3, 6, 12, 24 seconds\n",
    "                    log.warning(f\"  Attempt {attempt+1} failed ({e}). Retrying in {wait}s...\")\n",
    "                    time.sleep(wait)\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "        if \"error\" in data:\n",
    "            raise ValueError(f\"ArcGIS error: {data['error']}\")\n",
    "\n",
    "        features = data.get(\"features\", [])\n",
    "        if not features:\n",
    "            break\n",
    "\n",
    "        for feature in features:\n",
    "            row = feature.get(\"attributes\", {})\n",
    "            geom = feature.get(\"geometry\")\n",
    "            if geom:\n",
    "                if \"x\" in geom and \"y\" in geom:\n",
    "                    # Point geometry\n",
    "                    row[\"_longitude\"] = geom.get(\"x\")\n",
    "                    row[\"_latitude\"]  = geom.get(\"y\")\n",
    "                elif \"rings\" in geom:\n",
    "                    # Polygon geometry — capture outer ring as WKT\n",
    "                    rings = geom[\"rings\"]\n",
    "                    if rings:\n",
    "                        coords = \", \".join(f\"{x} {y}\" for x, y in reversed(rings[0]))\n",
    "                        row[\"geo_polygon_wkt\"] = f\"POLYGON(({coords}))\"\n",
    "            all_records.append(row)\n",
    "\n",
    "        log.info(f\"  {len(all_records):,} rows fetched...\")\n",
    "        offset += page_size\n",
    "\n",
    "        if len(features) < page_size:\n",
    "            break\n",
    "\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    df = pd.DataFrame.from_records(all_records)\n",
    "    log.info(f\"  ✓ {len(df):,} total rows.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_all_sources(config: dict) -> pd.DataFrame:\n",
    "    \"\"\"Fetch primary URL + any union_with URLs, concatenate into one DataFrame.\"\"\"\n",
    "    all_dfs = []\n",
    "    log.info(\"  Fetching primary source...\")\n",
    "    all_dfs.append(fetch_arcgis_layer(config[\"url\"]))\n",
    "\n",
    "    for i, union_url in enumerate(config.get(\"union_with\", [])):\n",
    "        log.info(f\"  Fetching union source {i+1}/{len(config['union_with'])}...\")\n",
    "        try:\n",
    "            all_dfs.append(fetch_arcgis_layer(union_url))\n",
    "        except Exception as e:\n",
    "            log.warning(f\"  ⚠ Union source {i+1} failed (skipping): {e}\")\n",
    "\n",
    "    combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    log.info(f\"  ✓ Combined: {len(combined):,} rows from {len(all_dfs)} source(s).\")\n",
    "    return combined\n",
    "\n",
    "\n",
    "def clean_dataframe(df: pd.DataFrame, config: dict) -> pd.DataFrame:\n",
    "    \"\"\"Normalize columns, parse dates, build geo WKT, add metadata.\"\"\"\n",
    "    df.columns = [c.lower().strip().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "    date_col = config.get(\"date_col\")\n",
    "    date_col_norm = date_col.lower() if date_col else None\n",
    "\n",
    "    if date_col_norm and date_col_norm in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[date_col_norm]):\n",
    "            df[date_col_norm] = pd.to_datetime(df[date_col_norm], unit=\"ms\", utc=True, errors=\"coerce\")\n",
    "        else:\n",
    "            df[date_col_norm] = pd.to_datetime(df[date_col_norm], utc=True, errors=\"coerce\")\n",
    "\n",
    "        # Use Int64 (nullable) to avoid int32 pyarrow conversion issues\n",
    "        df[\"_year\"]   = df[date_col_norm].dt.year.astype(\"Int64\")\n",
    "        df[\"_month\"]  = df[date_col_norm].dt.month.astype(\"Int64\")\n",
    "        df[\"_period\"] = df[\"_year\"].apply(\n",
    "            lambda y: \"pre_2021\" if pd.notna(y) and y < 2021 else \"post_2021\"\n",
    "        )\n",
    "\n",
    "    if \"_latitude\" in df.columns and \"_longitude\" in df.columns:\n",
    "        df[\"_latitude\"]  = pd.to_numeric(df[\"_latitude\"],  errors=\"coerce\")\n",
    "        df[\"_longitude\"] = pd.to_numeric(df[\"_longitude\"], errors=\"coerce\")\n",
    "        df[\"geo_point_wkt\"] = df.apply(\n",
    "            lambda r: f\"POINT({r['_longitude']} {r['_latitude']})\"\n",
    "            if pd.notna(r[\"_latitude\"]) and pd.notna(r[\"_longitude\"]) else None,\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    # Force mixed-type columns to string to avoid pyarrow conversion errors\n",
    "    if \"council_district\" in df.columns:\n",
    "        df[\"council_district\"] = df[\"council_district\"].astype(str)\n",
    "\n",
    "    # Use string for _ingested_at to avoid pyarrow UTC datetime issues\n",
    "    df[\"_ingested_at\"] = pd.Timestamp.utcnow().strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "    df[\"_source_url\"]  = config[\"url\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_bq_schema(df: pd.DataFrame, date_col: str = None) -> list:\n",
    "    \"\"\"Auto-generate BigQuery schema. Handles nullable int types and geo.\"\"\"\n",
    "    dtype_map = {\n",
    "        \"object\":              \"STRING\",\n",
    "        \"int64\":               \"INT64\",\n",
    "        \"Int64\":               \"INT64\",   # nullable integer\n",
    "        \"float64\":             \"FLOAT64\",\n",
    "        \"bool\":                \"BOOL\",\n",
    "        \"datetime64[ns]\":      \"TIMESTAMP\",\n",
    "        \"datetime64[ns, UTC]\": \"TIMESTAMP\",\n",
    "        \"datetime64[us, UTC]\": \"TIMESTAMP\",\n",
    "    }\n",
    "    schema = []\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        if col == \"geo_point_wkt\":\n",
    "            schema.append(bigquery.SchemaField(col, \"GEOGRAPHY\"))\n",
    "        elif col == \"geo_polygon_wkt\":\n",
    "            schema.append(bigquery.SchemaField(col, \"STRING\"))\n",
    "        elif date_col and col == date_col.lower():\n",
    "            schema.append(bigquery.SchemaField(col, \"TIMESTAMP\"))\n",
    "        else:\n",
    "            bq_type = dtype_map.get(str(dtype), \"STRING\")\n",
    "            schema.append(bigquery.SchemaField(col, bq_type))\n",
    "    return schema\n",
    "\n",
    "\n",
    "def load_to_bigquery(df: pd.DataFrame, table_name: str, date_col: str = None, partition_type: str = \"DAY\") -> None:\n",
    "    \"\"\"\n",
    "    Load DataFrame to BigQuery. Always WRITE_TRUNCATE.\n",
    "    Uses MONTH partitioning by default for long historical datasets.\n",
    "    Partitioned + clustered if date_col provided.\n",
    "    \"\"\"\n",
    "    table_ref  = f\"{GCP_PROJECT}.{BQ_DATASET}.{table_name}\"\n",
    "    schema     = build_bq_schema(df, date_col=date_col)\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=schema,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "    )\n",
    "    if date_col:\n",
    "        job_config.time_partitioning = bigquery.TimePartitioning(\n",
    "            type_=bigquery.TimePartitioningType.MONTH if partition_type == \"MONTH\" else bigquery.TimePartitioningType.DAY,\n",
    "            field=date_col.lower()\n",
    "        )\n",
    "        cluster_cols = [c for c in [\"_period\", \"neighborhood\", \"district\", \"new_district\"] if c in df.columns][:4]\n",
    "        if cluster_cols:\n",
    "            job_config.clustering_fields = cluster_cols\n",
    "\n",
    "    log.info(f\"  Loading to {table_ref}...\")\n",
    "    bq.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
    "    log.info(f\"  ✓ {bq.get_table(table_ref).num_rows:,} rows → {table_ref}\")\n",
    "\n",
    "\n",
    "print(\"✓ Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validate Endpoints\n",
    "Run this before the full ingestion. Confirms each endpoint is reachable and returns a record count.  \n",
    "Fix any ❌ before proceeding to Cell 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating endpoints...\n",
      "\n",
      "✅ crime_incidents_legacy                           644,737 records\n",
      "✅ crime_incidents_current                          239,435 records\n",
      "✅ bpd_arrests                                      393,475 records\n",
      "✅ vacant_building_notices                           11,990 records\n",
      "✅ vacant_building_rehabs                            12,123 records\n",
      "✅ building_permits                                 155,802 records\n",
      "✅   └─ union 1                                     274,367 records\n",
      "✅ service_requests_311                           1,009,100 records\n",
      "✅   └─ union 1                                     987,498 records\n",
      "✅   └─ union 2                                     852,787 records\n",
      "✅   └─ union 3                                     767,903 records\n",
      "✅   └─ union 4                                     792,440 records\n",
      "✅   └─ union 5                                     671,777 records\n",
      "✅   └─ union 6                                     698,099 records\n",
      "✅   └─ union 7                                     669,843 records\n",
      "✅   └─ union 8                                     609,569 records\n",
      "✅   └─ union 9                                     533,792 records\n",
      "✅   └─ union 10                                    503,449 records\n",
      "✅   └─ union 11                                    440,116 records\n",
      "✅   └─ union 12                                    469,534 records\n",
      "✅   └─ union 13                                        749 records\n",
      "✅   └─ union 14                                        948 records\n",
      "✅   └─ union 15                                        762 records\n",
      "✅   └─ union 16                                         21 records\n",
      "✅   └─ union 17                                         45 records\n",
      "✅   └─ union 18                                          6 records\n",
      "✅ real_property                                    238,496 records\n",
      "✅ neighborhood_boundaries                              279 records\n",
      "\n",
      "Done. Fix any ❌ URLs in Cell 1 before running the ingestion.\n"
     ]
    }
   ],
   "source": [
    "print(\"Validating endpoints...\\n\")\n",
    "\n",
    "for table_name, config in DATASETS.items():\n",
    "    all_urls = [config[\"url\"]] + config.get(\"union_with\", [])\n",
    "    for i, url in enumerate(all_urls):\n",
    "        label = table_name if i == 0 else f\"  └─ union {i}\"\n",
    "        try:\n",
    "            resp  = requests.get(url, params={\"where\": \"1=1\", \"returnCountOnly\": \"true\", \"f\": \"json\"}, timeout=15)\n",
    "            data  = resp.json()\n",
    "            if \"error\" in data:\n",
    "                print(f\"❌ {label:45} API error: {data['error'].get('message', '')}\")\n",
    "            else:\n",
    "                count = data.get('count', 'unknown')\n",
    "                print(f\"✅ {label:45} {count:>10,} records\" if isinstance(count, int) else f\"✅ {label:45} {count} records\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {label:45} {str(e)[:50]}\")\n",
    "\n",
    "print(\"\\nDone. Fix any ❌ URLs in Cell 1 before running the ingestion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Ingestion Pipeline\n",
    "Fetches all datasets and loads to BigQuery. Uses `WRITE_TRUNCATE` — safe to re-run.  \n",
    "**Expected runtime: 3-5 hours** (311 service requests is ~9M rows across 19 layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion_log = []\n",
    "\n",
    "for table_name, config in DATASETS.items():\n",
    "    log.info(f\"\\n{'='*60}\")\n",
    "    log.info(f\"Processing: {table_name}\")\n",
    "    if config.get(\"quality_note\"):\n",
    "        log.warning(f\"⚠ {config['quality_note']}\")\n",
    "\n",
    "    result = {\n",
    "        \"table\": table_name, \"status\": None, \"rows_ingested\": None,\n",
    "        \"null_rate_date_col\": None, \"null_rate_geo\": None,\n",
    "        \"date_range_min\": None, \"date_range_max\": None,\n",
    "        \"error\": None, \"quality_note\": config.get(\"quality_note\")\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        df            = fetch_all_sources(config)\n",
    "        df            = clean_dataframe(df, config)\n",
    "        date_col      = config.get(\"date_col\")\n",
    "        date_col_norm = date_col.lower() if date_col else None\n",
    "        partition_type = config.get(\"partition_type\", \"DAY\")\n",
    "\n",
    "        result[\"rows_ingested\"] = len(df)\n",
    "        if date_col_norm and date_col_norm in df.columns:\n",
    "            result[\"null_rate_date_col\"] = round(df[date_col_norm].isna().mean(), 4)\n",
    "            result[\"date_range_min\"]     = str(df[date_col_norm].min())\n",
    "            result[\"date_range_max\"]     = str(df[date_col_norm].max())\n",
    "        if \"geo_point_wkt\" in df.columns:\n",
    "            result[\"null_rate_geo\"] = round(df[\"geo_point_wkt\"].isna().mean(), 4)\n",
    "\n",
    "        load_to_bigquery(df, table_name, date_col=date_col_norm, partition_type=partition_type)\n",
    "        result[\"status\"] = \"SUCCESS\"\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(f\"✗ Failed: {table_name} — {e}\")\n",
    "        result[\"status\"] = \"FAILED\"\n",
    "        result[\"error\"]  = str(e)\n",
    "\n",
    "    ingestion_log.append(result)\n",
    "\n",
    "log.info(\"\\n✓ Ingestion pipeline complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ingestion Audit Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_df = pd.DataFrame(ingestion_log)\n",
    "pd.set_option(\"display.max_colwidth\", 60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INGESTION AUDIT REPORT\")\n",
    "print(f\"Run at: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC\")\n",
    "print(\"=\"*80)\n",
    "display(audit_df[[\"table\", \"status\", \"rows_ingested\", \"date_range_min\",\n",
    "                   \"date_range_max\", \"null_rate_date_col\", \"null_rate_geo\", \"quality_note\"]])\n",
    "\n",
    "# Append audit log to BigQuery for run history\n",
    "audit_df[\"run_at\"] = pd.Timestamp.utcnow()\n",
    "bq.load_table_from_dataframe(\n",
    "    audit_df,\n",
    "    f\"{GCP_PROJECT}.{BQ_DATASET}._ingestion_log\",\n",
    "    job_config=bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "        autodetect=True\n",
    "    )\n",
    ").result()\n",
    "print(f\"\\n✓ Audit log saved to {GCP_PROJECT}.{BQ_DATASET}._ingestion_log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Pre/Post 2021 Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTITIONED_TABLES = [\n",
    "    (\"crime_incidents_legacy\",  \"crimedatetime\"),\n",
    "    (\"crime_incidents_current\", \"crimedatetime\"),\n",
    "    (\"bpd_arrests\",             \"arrestdatetime\"),   # Note: not arrestdate\n",
    "    (\"vacant_building_notices\", \"datenotice\"),\n",
    "    (\"vacant_building_rehabs\",  \"dateissued\"),\n",
    "    (\"building_permits\",        \"issueddate\"),        # Note: not issuedate\n",
    "    (\"service_requests_311\",    \"createddate\"),\n",
    "]\n",
    "\n",
    "for table_name, date_col in PARTITIONED_TABLES:\n",
    "    for period, operator, year in [(\"pre_2021\", \"<\", 2021), (\"post_2021\", \">=\", 2021)]:\n",
    "        view_ref = f\"{GCP_PROJECT}.views.{table_name}_{period}\"\n",
    "        view     = bigquery.Table(view_ref)\n",
    "        view.view_query = f\"\"\"\n",
    "            SELECT * FROM `{GCP_PROJECT}.{BQ_DATASET}.{table_name}`\n",
    "            WHERE EXTRACT(YEAR FROM {date_col}) {operator} {year}\n",
    "        \"\"\"\n",
    "        bq.delete_table(view_ref, not_found_ok=True)\n",
    "        bq.create_table(view)\n",
    "        log.info(f\"  ✓ {view_ref}\")\n",
    "\n",
    "print(\"\\n✓ All pre/post 2021 views created in 'views' dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. BigQuery GIS Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geo coverage by period for each partitioned table\n",
    "for table_name in [t for t in DATASETS if DATASETS[t].get(\"date_col\")]:\n",
    "    try:\n",
    "        result = bq.query(f\"\"\"\n",
    "            SELECT _period,\n",
    "                   COUNT(*) AS total,\n",
    "                   COUNTIF(geo_point_wkt IS NOT NULL) AS with_geo,\n",
    "                   ROUND(COUNTIF(geo_point_wkt IS NOT NULL) / COUNT(*) * 100, 1) AS geo_pct\n",
    "            FROM `{GCP_PROJECT}.{BQ_DATASET}.{table_name}`\n",
    "            GROUP BY _period ORDER BY _period\n",
    "        \"\"\").to_dataframe()\n",
    "        print(f\"\\n{table_name}:\")\n",
    "        display(result)\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ {table_name}: {e}\")\n",
    "\n",
    "# Verify neighborhood_boundaries has polygon geometry\n",
    "print(\"\\nVerifying neighborhood_boundaries polygon geometry:\")\n",
    "try:\n",
    "    result = bq.query(f\"\"\"\n",
    "        SELECT\n",
    "            COUNT(*) as total,\n",
    "            COUNTIF(geo_polygon_wkt IS NOT NULL) as with_polygon\n",
    "        FROM `{GCP_PROJECT}.{BQ_DATASET}.neighborhood_boundaries`\n",
    "    \"\"\").to_dataframe()\n",
    "    row = result.iloc[0]\n",
    "    print(f\"  {int(row['with_polygon'])} / {int(row['total'])} neighborhoods have polygon geometry.\")\n",
    "except Exception as e:\n",
    "    print(f\"  ⚠ {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84efca18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 16:57:29,185 — INFO —   Fetching primary source...\n",
      "2026-02-26 16:57:29,544 — INFO —   Total records: 238496\n",
      "2026-02-26 16:57:30,378 — INFO —   1,000 rows fetched...\n",
      "2026-02-26 16:57:31,376 — INFO —   2,000 rows fetched...\n",
      "2026-02-26 16:57:32,467 — INFO —   3,000 rows fetched...\n",
      "2026-02-26 16:57:33,545 — INFO —   4,000 rows fetched...\n",
      "2026-02-26 16:57:34,589 — INFO —   5,000 rows fetched...\n",
      "2026-02-26 16:57:35,640 — INFO —   6,000 rows fetched...\n",
      "2026-02-26 16:57:36,755 — INFO —   7,000 rows fetched...\n",
      "2026-02-26 16:57:37,890 — INFO —   8,000 rows fetched...\n",
      "2026-02-26 16:57:38,993 — INFO —   9,000 rows fetched...\n",
      "2026-02-26 16:57:40,108 — INFO —   10,000 rows fetched...\n",
      "2026-02-26 16:57:41,207 — INFO —   11,000 rows fetched...\n",
      "2026-02-26 16:57:42,251 — INFO —   12,000 rows fetched...\n",
      "2026-02-26 16:57:43,285 — INFO —   13,000 rows fetched...\n",
      "2026-02-26 16:57:44,514 — INFO —   14,000 rows fetched...\n",
      "2026-02-26 16:57:45,642 — INFO —   15,000 rows fetched...\n",
      "2026-02-26 16:57:46,649 — INFO —   16,000 rows fetched...\n",
      "2026-02-26 16:57:47,701 — INFO —   17,000 rows fetched...\n",
      "2026-02-26 16:57:48,824 — INFO —   18,000 rows fetched...\n",
      "2026-02-26 16:57:49,889 — INFO —   19,000 rows fetched...\n",
      "2026-02-26 16:57:50,966 — INFO —   20,000 rows fetched...\n",
      "2026-02-26 16:57:52,092 — INFO —   21,000 rows fetched...\n",
      "2026-02-26 16:57:53,457 — INFO —   22,000 rows fetched...\n",
      "2026-02-26 16:57:54,632 — INFO —   23,000 rows fetched...\n",
      "2026-02-26 16:57:55,835 — INFO —   24,000 rows fetched...\n",
      "2026-02-26 16:57:56,962 — INFO —   25,000 rows fetched...\n",
      "2026-02-26 16:57:58,152 — INFO —   26,000 rows fetched...\n",
      "2026-02-26 16:57:59,521 — INFO —   27,000 rows fetched...\n",
      "2026-02-26 16:58:00,543 — INFO —   28,000 rows fetched...\n",
      "2026-02-26 16:58:01,652 — INFO —   29,000 rows fetched...\n",
      "2026-02-26 16:58:02,771 — INFO —   30,000 rows fetched...\n",
      "2026-02-26 16:58:03,887 — INFO —   31,000 rows fetched...\n",
      "2026-02-26 16:58:04,985 — INFO —   32,000 rows fetched...\n",
      "2026-02-26 16:58:06,145 — INFO —   33,000 rows fetched...\n",
      "2026-02-26 16:58:07,284 — INFO —   34,000 rows fetched...\n",
      "2026-02-26 16:58:08,375 — INFO —   35,000 rows fetched...\n",
      "2026-02-26 16:58:09,518 — INFO —   36,000 rows fetched...\n",
      "2026-02-26 16:58:10,607 — INFO —   37,000 rows fetched...\n",
      "2026-02-26 16:58:11,731 — INFO —   38,000 rows fetched...\n",
      "2026-02-26 16:58:12,817 — INFO —   39,000 rows fetched...\n",
      "2026-02-26 16:58:14,046 — INFO —   40,000 rows fetched...\n",
      "2026-02-26 16:58:15,167 — INFO —   41,000 rows fetched...\n",
      "2026-02-26 16:58:16,329 — INFO —   42,000 rows fetched...\n",
      "2026-02-26 16:58:17,378 — INFO —   43,000 rows fetched...\n",
      "2026-02-26 16:58:18,607 — INFO —   44,000 rows fetched...\n",
      "2026-02-26 16:58:20,075 — INFO —   45,000 rows fetched...\n",
      "2026-02-26 16:58:21,402 — INFO —   46,000 rows fetched...\n",
      "2026-02-26 16:58:22,552 — INFO —   47,000 rows fetched...\n",
      "2026-02-26 16:58:23,637 — INFO —   48,000 rows fetched...\n",
      "2026-02-26 16:58:24,745 — INFO —   49,000 rows fetched...\n",
      "2026-02-26 16:58:25,797 — INFO —   50,000 rows fetched...\n",
      "2026-02-26 16:58:26,919 — INFO —   51,000 rows fetched...\n",
      "2026-02-26 16:58:28,019 — INFO —   52,000 rows fetched...\n",
      "2026-02-26 16:58:29,116 — INFO —   53,000 rows fetched...\n",
      "2026-02-26 16:58:30,198 — INFO —   54,000 rows fetched...\n",
      "2026-02-26 16:58:31,307 — INFO —   55,000 rows fetched...\n",
      "2026-02-26 16:58:32,471 — INFO —   56,000 rows fetched...\n",
      "2026-02-26 16:58:33,462 — INFO —   57,000 rows fetched...\n",
      "2026-02-26 16:58:34,550 — INFO —   58,000 rows fetched...\n",
      "2026-02-26 16:58:35,630 — INFO —   59,000 rows fetched...\n",
      "2026-02-26 16:58:36,708 — INFO —   60,000 rows fetched...\n",
      "2026-02-26 16:58:37,808 — INFO —   61,000 rows fetched...\n",
      "2026-02-26 16:58:38,898 — INFO —   62,000 rows fetched...\n",
      "2026-02-26 16:58:39,921 — INFO —   63,000 rows fetched...\n",
      "2026-02-26 16:58:41,141 — INFO —   64,000 rows fetched...\n",
      "2026-02-26 16:58:42,296 — INFO —   65,000 rows fetched...\n",
      "2026-02-26 16:58:43,344 — INFO —   66,000 rows fetched...\n",
      "2026-02-26 16:58:44,472 — INFO —   67,000 rows fetched...\n",
      "2026-02-26 16:58:45,680 — INFO —   68,000 rows fetched...\n",
      "2026-02-26 16:58:46,799 — INFO —   69,000 rows fetched...\n",
      "2026-02-26 16:58:48,150 — INFO —   70,000 rows fetched...\n",
      "2026-02-26 16:58:49,535 — INFO —   71,000 rows fetched...\n",
      "2026-02-26 16:58:50,660 — INFO —   72,000 rows fetched...\n",
      "2026-02-26 16:58:51,658 — INFO —   73,000 rows fetched...\n",
      "2026-02-26 16:58:52,724 — INFO —   74,000 rows fetched...\n",
      "2026-02-26 16:58:53,844 — INFO —   75,000 rows fetched...\n",
      "2026-02-26 16:58:55,155 — INFO —   76,000 rows fetched...\n",
      "2026-02-26 16:58:56,222 — INFO —   77,000 rows fetched...\n",
      "2026-02-26 16:58:57,410 — INFO —   78,000 rows fetched...\n",
      "2026-02-26 16:58:58,518 — INFO —   79,000 rows fetched...\n",
      "2026-02-26 16:58:59,626 — INFO —   80,000 rows fetched...\n",
      "2026-02-26 16:59:00,830 — INFO —   81,000 rows fetched...\n",
      "2026-02-26 16:59:01,941 — INFO —   82,000 rows fetched...\n",
      "2026-02-26 16:59:03,046 — INFO —   83,000 rows fetched...\n",
      "2026-02-26 16:59:04,172 — INFO —   84,000 rows fetched...\n",
      "2026-02-26 16:59:05,276 — INFO —   85,000 rows fetched...\n",
      "2026-02-26 16:59:06,453 — INFO —   86,000 rows fetched...\n",
      "2026-02-26 16:59:07,603 — INFO —   87,000 rows fetched...\n",
      "2026-02-26 16:59:08,941 — INFO —   88,000 rows fetched...\n",
      "2026-02-26 16:59:10,028 — INFO —   89,000 rows fetched...\n",
      "2026-02-26 16:59:11,070 — INFO —   90,000 rows fetched...\n",
      "2026-02-26 16:59:12,083 — INFO —   91,000 rows fetched...\n",
      "2026-02-26 16:59:13,183 — INFO —   92,000 rows fetched...\n",
      "2026-02-26 16:59:14,319 — INFO —   93,000 rows fetched...\n",
      "2026-02-26 16:59:15,371 — INFO —   94,000 rows fetched...\n",
      "2026-02-26 16:59:16,493 — INFO —   95,000 rows fetched...\n",
      "2026-02-26 16:59:17,556 — INFO —   96,000 rows fetched...\n",
      "2026-02-26 16:59:18,568 — INFO —   97,000 rows fetched...\n",
      "2026-02-26 16:59:19,679 — INFO —   98,000 rows fetched...\n",
      "2026-02-26 16:59:20,790 — INFO —   99,000 rows fetched...\n",
      "2026-02-26 16:59:21,837 — INFO —   100,000 rows fetched...\n",
      "2026-02-26 16:59:22,890 — INFO —   101,000 rows fetched...\n",
      "2026-02-26 16:59:24,002 — INFO —   102,000 rows fetched...\n",
      "2026-02-26 16:59:25,083 — INFO —   103,000 rows fetched...\n",
      "2026-02-26 16:59:26,157 — INFO —   104,000 rows fetched...\n",
      "2026-02-26 16:59:27,263 — INFO —   105,000 rows fetched...\n",
      "2026-02-26 16:59:28,393 — INFO —   106,000 rows fetched...\n",
      "2026-02-26 16:59:29,541 — INFO —   107,000 rows fetched...\n",
      "2026-02-26 16:59:30,640 — INFO —   108,000 rows fetched...\n",
      "2026-02-26 16:59:31,711 — INFO —   109,000 rows fetched...\n",
      "2026-02-26 16:59:32,849 — INFO —   110,000 rows fetched...\n",
      "2026-02-26 16:59:33,949 — INFO —   111,000 rows fetched...\n",
      "2026-02-26 16:59:34,990 — INFO —   112,000 rows fetched...\n",
      "2026-02-26 16:59:36,099 — INFO —   113,000 rows fetched...\n",
      "2026-02-26 16:59:37,131 — INFO —   114,000 rows fetched...\n",
      "2026-02-26 16:59:38,197 — INFO —   115,000 rows fetched...\n",
      "2026-02-26 16:59:39,338 — INFO —   116,000 rows fetched...\n",
      "2026-02-26 16:59:40,444 — INFO —   117,000 rows fetched...\n",
      "2026-02-26 16:59:41,555 — INFO —   118,000 rows fetched...\n",
      "2026-02-26 16:59:42,888 — INFO —   119,000 rows fetched...\n",
      "2026-02-26 16:59:44,224 — INFO —   120,000 rows fetched...\n",
      "2026-02-26 16:59:45,429 — INFO —   121,000 rows fetched...\n",
      "2026-02-26 16:59:46,646 — INFO —   122,000 rows fetched...\n",
      "2026-02-26 16:59:47,752 — INFO —   123,000 rows fetched...\n",
      "2026-02-26 16:59:48,955 — INFO —   124,000 rows fetched...\n",
      "2026-02-26 16:59:50,057 — INFO —   125,000 rows fetched...\n",
      "2026-02-26 16:59:51,229 — INFO —   126,000 rows fetched...\n",
      "2026-02-26 16:59:52,316 — INFO —   127,000 rows fetched...\n",
      "2026-02-26 16:59:53,450 — INFO —   128,000 rows fetched...\n",
      "2026-02-26 16:59:54,590 — INFO —   129,000 rows fetched...\n",
      "2026-02-26 16:59:55,680 — INFO —   130,000 rows fetched...\n",
      "2026-02-26 16:59:56,821 — INFO —   131,000 rows fetched...\n",
      "2026-02-26 16:59:57,937 — INFO —   132,000 rows fetched...\n",
      "2026-02-26 16:59:59,109 — INFO —   133,000 rows fetched...\n",
      "2026-02-26 17:00:00,257 — INFO —   134,000 rows fetched...\n",
      "2026-02-26 17:00:01,430 — INFO —   135,000 rows fetched...\n",
      "2026-02-26 17:00:02,546 — INFO —   136,000 rows fetched...\n",
      "2026-02-26 17:00:03,654 — INFO —   137,000 rows fetched...\n",
      "2026-02-26 17:00:04,708 — INFO —   138,000 rows fetched...\n",
      "2026-02-26 17:00:05,848 — INFO —   139,000 rows fetched...\n",
      "2026-02-26 17:00:06,939 — INFO —   140,000 rows fetched...\n",
      "2026-02-26 17:00:08,117 — INFO —   141,000 rows fetched...\n",
      "2026-02-26 17:00:09,434 — INFO —   142,000 rows fetched...\n",
      "2026-02-26 17:00:10,547 — INFO —   143,000 rows fetched...\n",
      "2026-02-26 17:00:12,123 — INFO —   144,000 rows fetched...\n",
      "2026-02-26 17:00:13,352 — INFO —   145,000 rows fetched...\n",
      "2026-02-26 17:00:14,486 — INFO —   146,000 rows fetched...\n",
      "2026-02-26 17:00:15,688 — INFO —   147,000 rows fetched...\n",
      "2026-02-26 17:00:17,347 — INFO —   148,000 rows fetched...\n",
      "2026-02-26 17:00:18,524 — INFO —   149,000 rows fetched...\n",
      "2026-02-26 17:00:19,731 — INFO —   150,000 rows fetched...\n",
      "2026-02-26 17:00:20,894 — INFO —   151,000 rows fetched...\n",
      "2026-02-26 17:00:22,312 — INFO —   152,000 rows fetched...\n",
      "2026-02-26 17:00:23,510 — INFO —   153,000 rows fetched...\n",
      "2026-02-26 17:00:24,692 — INFO —   154,000 rows fetched...\n",
      "2026-02-26 17:00:25,768 — INFO —   155,000 rows fetched...\n",
      "2026-02-26 17:00:26,906 — INFO —   156,000 rows fetched...\n",
      "2026-02-26 17:00:28,067 — INFO —   157,000 rows fetched...\n",
      "2026-02-26 17:00:29,188 — INFO —   158,000 rows fetched...\n",
      "2026-02-26 17:00:30,498 — INFO —   159,000 rows fetched...\n",
      "2026-02-26 17:00:31,603 — INFO —   160,000 rows fetched...\n",
      "2026-02-26 17:00:32,842 — INFO —   161,000 rows fetched...\n",
      "2026-02-26 17:00:34,268 — INFO —   162,000 rows fetched...\n",
      "2026-02-26 17:00:35,384 — INFO —   163,000 rows fetched...\n",
      "2026-02-26 17:00:36,477 — INFO —   164,000 rows fetched...\n",
      "2026-02-26 17:00:37,685 — INFO —   165,000 rows fetched...\n",
      "2026-02-26 17:00:38,792 — INFO —   166,000 rows fetched...\n",
      "2026-02-26 17:00:39,981 — INFO —   167,000 rows fetched...\n",
      "2026-02-26 17:00:41,110 — INFO —   168,000 rows fetched...\n",
      "2026-02-26 17:00:42,224 — INFO —   169,000 rows fetched...\n",
      "2026-02-26 17:00:43,315 — INFO —   170,000 rows fetched...\n",
      "2026-02-26 17:00:44,437 — INFO —   171,000 rows fetched...\n",
      "2026-02-26 17:00:45,552 — INFO —   172,000 rows fetched...\n",
      "2026-02-26 17:00:46,782 — INFO —   173,000 rows fetched...\n",
      "2026-02-26 17:00:47,852 — INFO —   174,000 rows fetched...\n",
      "2026-02-26 17:00:48,969 — INFO —   175,000 rows fetched...\n",
      "2026-02-26 17:00:50,038 — INFO —   176,000 rows fetched...\n",
      "2026-02-26 17:00:51,230 — INFO —   177,000 rows fetched...\n",
      "2026-02-26 17:00:52,631 — INFO —   178,000 rows fetched...\n",
      "2026-02-26 17:00:53,707 — INFO —   179,000 rows fetched...\n",
      "2026-02-26 17:00:54,799 — INFO —   180,000 rows fetched...\n",
      "2026-02-26 17:00:55,878 — INFO —   181,000 rows fetched...\n",
      "2026-02-26 17:00:56,987 — INFO —   182,000 rows fetched...\n",
      "2026-02-26 17:00:58,090 — INFO —   183,000 rows fetched...\n",
      "2026-02-26 17:00:59,260 — INFO —   184,000 rows fetched...\n",
      "2026-02-26 17:01:00,378 — INFO —   185,000 rows fetched...\n",
      "2026-02-26 17:01:01,528 — INFO —   186,000 rows fetched...\n",
      "2026-02-26 17:01:02,617 — INFO —   187,000 rows fetched...\n",
      "2026-02-26 17:01:03,751 — INFO —   188,000 rows fetched...\n",
      "2026-02-26 17:01:04,932 — INFO —   189,000 rows fetched...\n",
      "2026-02-26 17:01:06,042 — INFO —   190,000 rows fetched...\n",
      "2026-02-26 17:01:07,226 — INFO —   191,000 rows fetched...\n",
      "2026-02-26 17:01:08,388 — INFO —   192,000 rows fetched...\n",
      "2026-02-26 17:01:09,591 — INFO —   193,000 rows fetched...\n",
      "2026-02-26 17:01:10,734 — INFO —   194,000 rows fetched...\n",
      "2026-02-26 17:01:11,807 — INFO —   195,000 rows fetched...\n",
      "2026-02-26 17:01:12,998 — INFO —   196,000 rows fetched...\n",
      "2026-02-26 17:01:14,160 — INFO —   197,000 rows fetched...\n",
      "2026-02-26 17:01:15,367 — INFO —   198,000 rows fetched...\n",
      "2026-02-26 17:01:16,528 — INFO —   199,000 rows fetched...\n",
      "2026-02-26 17:01:17,621 — INFO —   200,000 rows fetched...\n",
      "2026-02-26 17:01:18,753 — INFO —   201,000 rows fetched...\n",
      "2026-02-26 17:01:19,836 — INFO —   202,000 rows fetched...\n",
      "2026-02-26 17:01:20,951 — INFO —   203,000 rows fetched...\n",
      "2026-02-26 17:01:22,594 — INFO —   204,000 rows fetched...\n",
      "2026-02-26 17:01:24,168 — INFO —   205,000 rows fetched...\n",
      "2026-02-26 17:01:25,312 — INFO —   206,000 rows fetched...\n",
      "2026-02-26 17:01:26,482 — INFO —   207,000 rows fetched...\n",
      "2026-02-26 17:01:27,672 — INFO —   208,000 rows fetched...\n",
      "2026-02-26 17:01:28,839 — INFO —   209,000 rows fetched...\n",
      "2026-02-26 17:01:30,227 — INFO —   210,000 rows fetched...\n",
      "2026-02-26 17:01:31,339 — INFO —   211,000 rows fetched...\n",
      "2026-02-26 17:01:32,471 — INFO —   212,000 rows fetched...\n",
      "2026-02-26 17:01:33,684 — INFO —   213,000 rows fetched...\n",
      "2026-02-26 17:01:34,813 — INFO —   214,000 rows fetched...\n",
      "2026-02-26 17:01:35,937 — INFO —   215,000 rows fetched...\n",
      "2026-02-26 17:01:37,057 — INFO —   216,000 rows fetched...\n",
      "2026-02-26 17:01:38,180 — INFO —   217,000 rows fetched...\n",
      "2026-02-26 17:01:39,341 — INFO —   218,000 rows fetched...\n",
      "2026-02-26 17:01:40,462 — INFO —   219,000 rows fetched...\n",
      "2026-02-26 17:01:41,882 — INFO —   220,000 rows fetched...\n",
      "2026-02-26 17:01:43,052 — INFO —   221,000 rows fetched...\n",
      "2026-02-26 17:01:44,216 — INFO —   222,000 rows fetched...\n",
      "2026-02-26 17:01:45,405 — INFO —   223,000 rows fetched...\n",
      "2026-02-26 17:01:46,535 — INFO —   224,000 rows fetched...\n",
      "2026-02-26 17:01:47,657 — INFO —   225,000 rows fetched...\n",
      "2026-02-26 17:01:48,784 — INFO —   226,000 rows fetched...\n",
      "2026-02-26 17:01:49,849 — INFO —   227,000 rows fetched...\n",
      "2026-02-26 17:01:51,089 — INFO —   228,000 rows fetched...\n",
      "2026-02-26 17:01:52,234 — INFO —   229,000 rows fetched...\n",
      "2026-02-26 17:01:53,432 — INFO —   230,000 rows fetched...\n",
      "2026-02-26 17:01:54,605 — INFO —   231,000 rows fetched...\n",
      "2026-02-26 17:01:55,739 — INFO —   232,000 rows fetched...\n",
      "2026-02-26 17:01:56,908 — INFO —   233,000 rows fetched...\n",
      "2026-02-26 17:01:58,161 — INFO —   234,000 rows fetched...\n",
      "2026-02-26 17:01:59,347 — INFO —   235,000 rows fetched...\n",
      "2026-02-26 17:02:00,465 — INFO —   236,000 rows fetched...\n",
      "2026-02-26 17:02:01,560 — INFO —   237,000 rows fetched...\n",
      "2026-02-26 17:02:02,787 — INFO —   238,000 rows fetched...\n",
      "2026-02-26 17:02:03,835 — INFO —   238,496 rows fetched...\n",
      "2026-02-26 17:02:05,787 — INFO —   ✓ 238,496 total rows.\n",
      "2026-02-26 17:02:05,957 — INFO —   ✓ Combined: 238,496 rows from 1 source(s).\n",
      "2026-02-26 17:02:05,960 — INFO —   Loading to baltimore-analytics.raw_data.real_property...\n",
      "c:\\Users\\spenc\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py:486: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n",
      "2026-02-26 17:02:25,751 — INFO —   ✓ 238,496 rows → baltimore-analytics.raw_data.real_property\n"
     ]
    }
   ],
   "source": [
    "bq.delete_table(\"baltimore-analytics.raw_data.real_property\", not_found_ok=True)\n",
    "config = DATASETS[\"real_property\"]\n",
    "df = fetch_all_sources(config)\n",
    "df = clean_dataframe(df, config)\n",
    "load_to_bigquery(df, \"real_property\", date_col=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "Once all datasets show `SUCCESS` and geo coverage looks reasonable:\n",
    "\n",
    "1. **`02_spatial_enrichment.ipynb`** — Spatial join all point tables to NSA polygons, standardize `neighborhood` as universal join key\n",
    "2. **`03_feature_engineering.ipynb`** — Aggregate to NSA level, build Neighborhood Vitality Index feature matrix\n",
    "3. **`04_clustering.ipynb`** — K-means segmentation of neighborhoods into cohorts\n",
    "4. **`05_looker_studio.md`** — Dashboard connection guide\n",
    "\n",
    "**If 311 annual union endpoints fail:**  \n",
    "Search `data.baltimorecity.gov` for `311 2015` (etc.), grab the URL, click the API tab to get the correct FeatureServer endpoint, and update `union_with` in Cell 1.\n",
    "\n",
    "**Known data quality issues:**\n",
    "- `bpd_arrests`: ~41% of records missing geo (older records never geocoded by BPD)\n",
    "- `service_requests_311`: ~13% missing geo (older pre-2010 records)\n",
    "- `service_requests_311`: 2023-2026 data excluded — consolidated endpoint unavailable\n",
    "\n",
    "**BigQuery GIS quick reference:**\n",
    "```sql\n",
    "-- Point in polygon\n",
    "ST_WITHIN(point_geo, polygon_geo)\n",
    "\n",
    "-- Distance between points (meters)\n",
    "ST_DISTANCE(point_a, point_b)\n",
    "\n",
    "-- Count points within polygon\n",
    "ST_INTERSECTS(point_geo, polygon_geo)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
