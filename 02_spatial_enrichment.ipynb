{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell_title",
   "metadata": {},
   "source": [
    "# Baltimore Analytics — Spatial Enrichment Pipeline\n",
    "**Project:** `baltimore-analytics`  \n",
    "**Input:** `raw_data` dataset  \n",
    "**Output:** `analytics` dataset  \n",
    "**Author:** Spencer  \n",
    "\n",
    "---\n",
    "### What This Notebook Does\n",
    "Spatially joins all point tables to `neighborhood_boundaries` polygons using BigQuery GIS (`ST_WITHIN`).  \n",
    "Adds a standardized `neighborhood` column to each table as the universal join key.\n",
    "\n",
    "```\n",
    "raw_data.<table>  +  raw_data.neighborhood_boundaries  →  analytics.<table>\n",
    "```\n",
    "\n",
    "### Tables Enriched\n",
    "| Table | Date Col | Rows (approx) | Geo Coverage |\n",
    "|---|---|---|---|\n",
    "| crime_incidents_legacy | crimedatetime | 644,737 | 99.8% |\n",
    "| crime_incidents_current | crimedatetime | 239,435 | 99.9% |\n",
    "| bpd_arrests | arrestdatetime | 393,475 | 58.8% |\n",
    "| vacant_building_notices | datenotice | 11,990 | 100.0% |\n",
    "| vacant_building_rehabs | dateissued | 12,123 | 100.0% |\n",
    "| building_permits | issueddate | 430,169 | 100.0% |\n",
    "| service_requests_311 | createddate | 9,008,438 | 86.8% |\n",
    "\n",
    "**Note:** Records without coordinates will have `neighborhood = NULL` after the join.  \n",
    "This is expected — see geo coverage above.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_s0",
   "metadata": {},
   "source": [
    "## 0. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Config loaded. 7 point tables to enrich, 2 reference tables to copy.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s — %(levelname)s — %(message)s')\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "GCP_PROJECT      = \"baltimore-analytics\"\n",
    "RAW_DATASET      = \"raw_data\"\n",
    "ANALYTICS_DATASET = \"analytics\"\n",
    "GCP_REGION       = \"us-east1\"\n",
    "CREDENTIALS_PATH = \"service_account.json\"\n",
    "\n",
    "# Point tables to enrich — (raw_table_name, output_table_name)\n",
    "POINT_TABLES = [\n",
    "    \"crime_incidents_legacy\",\n",
    "    \"crime_incidents_current\",\n",
    "    \"bpd_arrests\",\n",
    "    \"vacant_building_notices\",\n",
    "    \"vacant_building_rehabs\",\n",
    "    \"building_permits\",\n",
    "    \"service_requests_311\",\n",
    "]\n",
    "\n",
    "# Reference tables — copied as-is to analytics (no spatial join needed)\n",
    "REFERENCE_TABLES = [\n",
    "    \"real_property\",\n",
    "    \"neighborhood_boundaries\",\n",
    "]\n",
    "\n",
    "print(f\"✓ Config loaded. {len(POINT_TABLES)} point tables to enrich, {len(REFERENCE_TABLES)} reference tables to copy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_s1",
   "metadata": {},
   "source": [
    "## 1. Initialize Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell_client",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 16:37:55,463 — INFO — ✓ Dataset 'baltimore-analytics.analytics' ready.\n"
     ]
    }
   ],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    CREDENTIALS_PATH,\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    ")\n",
    "bq = bigquery.Client(project=GCP_PROJECT, credentials=credentials)\n",
    "\n",
    "# Ensure analytics dataset exists\n",
    "ds = bigquery.Dataset(f\"{GCP_PROJECT}.{ANALYTICS_DATASET}\")\n",
    "ds.location = GCP_REGION\n",
    "bq.create_dataset(ds, exists_ok=True)\n",
    "log.info(f\"✓ Dataset '{GCP_PROJECT}.{ANALYTICS_DATASET}' ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_s2",
   "metadata": {},
   "source": [
    "## 2. Pre-Flight Checks\n",
    "Verify all source tables exist and `neighborhood_boundaries` has polygon geometry before running enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell_preflight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking source tables...\n",
      "\n",
      "✅ crime_incidents_legacy: 644,737 rows\n",
      "✅ crime_incidents_current: 239,435 rows\n",
      "✅ bpd_arrests: 393,475 rows\n",
      "✅ vacant_building_notices: 11,990 rows\n",
      "✅ vacant_building_rehabs: 12,123 rows\n",
      "✅ building_permits: 430,169 rows\n",
      "✅ service_requests_311: 9,008,438 rows\n",
      "✅ real_property: 238,496 rows\n",
      "✅ neighborhood_boundaries: 279 rows\n",
      "\n",
      "Checking neighborhood_boundaries polygon geometry...\n",
      "✅ All 279 neighborhoods have polygon geometry.\n",
      "\n",
      "✓ All checks passed. Ready to run enrichment.\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking source tables...\\n\")\n",
    "\n",
    "all_ok = True\n",
    "for table_name in POINT_TABLES + REFERENCE_TABLES:\n",
    "    try:\n",
    "        t = bq.get_table(f\"{GCP_PROJECT}.{RAW_DATASET}.{table_name}\")\n",
    "        print(f\"✅ {table_name}: {t.num_rows:,} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {table_name}: {e}\")\n",
    "        all_ok = False\n",
    "\n",
    "# Verify neighborhood_boundaries has polygon geometry\n",
    "print(\"\\nChecking neighborhood_boundaries polygon geometry...\")\n",
    "try:\n",
    "    result = bq.query(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total,\n",
    "            COUNTIF(geo_polygon_wkt IS NOT NULL) as with_polygon\n",
    "        FROM `baltimore-analytics.raw_data.neighborhood_boundaries`\n",
    "    \"\"\").to_dataframe()\n",
    "    row = result.iloc[0]\n",
    "    if row['with_polygon'] == row['total']:\n",
    "        print(f\"✅ All {int(row['total'])} neighborhoods have polygon geometry.\")\n",
    "    else:\n",
    "        print(f\"⚠ Only {int(row['with_polygon'])} / {int(row['total'])} neighborhoods have polygon geometry.\")\n",
    "        all_ok = False\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not verify polygon geometry: {e}\")\n",
    "    all_ok = False\n",
    "\n",
    "print(f\"\\n{'✓ All checks passed. Ready to run enrichment.' if all_ok else '✗ Fix issues above before proceeding.'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_s3",
   "metadata": {},
   "source": [
    "## 3. Spatial Enrichment\n",
    "For each point table: LEFT JOIN to `neighborhood_boundaries` using `ST_WITHIN`,  \n",
    "write result to `analytics` dataset. Uses `WRITE_TRUNCATE` — safe to re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_enrich",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:39:30,784 — INFO — \n",
      "============================================================\n",
      "2026-02-26 15:39:30,785 — INFO — Enriching: crime_incidents_legacy\n",
      "2026-02-26 15:39:31,330 — ERROR —   ✗ Failed: crime_incidents_legacy — 400 Column objectid in SELECT * EXCEPT list does not exist at [3:25]; reason: invalidQuery, location: query, message: Column objectid in SELECT * EXCEPT list does not exist at [3:25]\n",
      "\n",
      "Location: us-east1\n",
      "Job ID: 4b4a8c05-09d6-4c3b-89f1-92e4f8613dec\n",
      "\n",
      "2026-02-26 15:39:31,331 — INFO — \n",
      "============================================================\n",
      "2026-02-26 15:39:31,331 — INFO — Enriching: crime_incidents_current\n",
      "2026-02-26 15:39:31,761 — ERROR —   ✗ Failed: crime_incidents_current — 400 Column objectid in SELECT * EXCEPT list does not exist at [3:25]; reason: invalidQuery, location: query, message: Column objectid in SELECT * EXCEPT list does not exist at [3:25]\n",
      "\n",
      "Location: us-east1\n",
      "Job ID: c00effd1-957a-4a70-a591-1dfbe680519c\n",
      "\n",
      "2026-02-26 15:39:31,762 — INFO — \n",
      "============================================================\n",
      "2026-02-26 15:39:31,762 — INFO — Enriching: bpd_arrests\n",
      "2026-02-26 15:39:32,140 — ERROR —   ✗ Failed: bpd_arrests — 400 Column objectid in SELECT * EXCEPT list does not exist at [3:25]; reason: invalidQuery, location: query, message: Column objectid in SELECT * EXCEPT list does not exist at [3:25]\n",
      "\n",
      "Location: us-east1\n",
      "Job ID: 08861837-6b8c-43d5-a48f-993a8b4b7b41\n",
      "\n",
      "2026-02-26 15:39:32,140 — INFO — \n",
      "============================================================\n",
      "2026-02-26 15:39:32,141 — INFO — Enriching: vacant_building_notices\n",
      "2026-02-26 15:39:32,577 — ERROR —   ✗ Failed: vacant_building_notices — 400 Duplicate column names in the result are not supported when a destination table is present. Found duplicate(s): neighborhood; reason: invalidQuery, location: query, message: Duplicate column names in the result are not supported when a destination table is present. Found duplicate(s): neighborhood\n",
      "\n",
      "Location: us-east1\n",
      "Job ID: a03df6fd-1a60-419e-baac-f97987dd2663\n",
      "\n",
      "2026-02-26 15:39:32,578 — INFO — \n",
      "============================================================\n",
      "2026-02-26 15:39:32,578 — INFO — Enriching: vacant_building_rehabs\n",
      "2026-02-26 15:39:32,948 — ERROR —   ✗ Failed: vacant_building_rehabs — 400 Duplicate column names in the result are not supported when a destination table is present. Found duplicate(s): neighborhood; reason: invalidQuery, location: query, message: Duplicate column names in the result are not supported when a destination table is present. Found duplicate(s): neighborhood\n",
      "\n",
      "Location: us-east1\n",
      "Job ID: 222b7786-ce7c-40db-a8ae-cd764ef4e8bb\n",
      "\n",
      "2026-02-26 15:39:32,949 — INFO — \n",
      "============================================================\n",
      "2026-02-26 15:39:32,950 — INFO — Enriching: building_permits\n",
      "2026-02-26 15:39:33,317 — ERROR —   ✗ Failed: building_permits — 400 Duplicate column names in the result are not supported when a destination table is present. Found duplicate(s): neighborhood; reason: invalidQuery, location: query, message: Duplicate column names in the result are not supported when a destination table is present. Found duplicate(s): neighborhood\n",
      "\n",
      "Location: us-east1\n",
      "Job ID: 62ad4245-2fdf-48a3-8937-d44beca6443d\n",
      "\n",
      "2026-02-26 15:39:33,319 — INFO — \n",
      "============================================================\n",
      "2026-02-26 15:39:33,320 — INFO — Enriching: service_requests_311\n",
      "2026-02-26 15:39:33,734 — ERROR —   ✗ Failed: service_requests_311 — 400 Column objectid in SELECT * EXCEPT list does not exist at [3:25]; reason: invalidQuery, location: query, message: Column objectid in SELECT * EXCEPT list does not exist at [3:25]\n",
      "\n",
      "Location: us-east1\n",
      "Job ID: 10fb9b11-7c8e-4442-bcbc-88d4b9f56a29\n",
      "\n",
      "2026-02-26 15:39:33,735 — INFO — \n",
      "✓ Spatial enrichment complete.\n"
     ]
    }
   ],
   "source": [
    "enrichment_log = []\n",
    "\n",
    "for table_name in POINT_TABLES:\n",
    "    log.info(f\"\\n{'='*60}\")\n",
    "    log.info(f\"Enriching: {table_name}\")\n",
    "\n",
    "    result = {\"table\": table_name, \"status\": None, \"rows_total\": None,\n",
    "              \"rows_with_neighborhood\": None, \"match_rate\": None, \"error\": None}\n",
    "\n",
    "    try:\n",
    "        src  = f\"`{GCP_PROJECT}.{RAW_DATASET}.{table_name}`\"\n",
    "        nsa  = f\"`{GCP_PROJECT}.{RAW_DATASET}.neighborhood_boundaries`\"\n",
    "        dest = f\"{GCP_PROJECT}.{ANALYTICS_DATASET}.{table_name}\"\n",
    "\n",
    "        # Dynamically build EXCEPT list based on columns that conflict with the join output\n",
    "        src_table = bq.get_table(f\"{GCP_PROJECT}.{RAW_DATASET}.{table_name}\")\n",
    "        src_cols = [f.name for f in src_table.schema]\n",
    "        except_cols = [c for c in [\"objectid\", \"neighborhood\", \"nsa_population\", \"nsa_objectid\", \"_ingested_at\", \"_source_url\"] if c in src_cols]\n",
    "        except_clause = f\"EXCEPT ({', '.join(except_cols)})\" if except_cols else \"\"\n",
    "\n",
    "        sql = f\"\"\"\n",
    "            SELECT\n",
    "                p.* {except_clause},\n",
    "                n.name        AS neighborhood,\n",
    "                n.population  AS nsa_population,\n",
    "                n.objectid    AS nsa_objectid,\n",
    "                '{pd.Timestamp.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}' AS _ingested_at,\n",
    "                p._source_url\n",
    "            FROM {src} p\n",
    "            LEFT JOIN {nsa} n\n",
    "            ON ST_WITHIN(p.geo_point_wkt, n.geo_polygon_wkt)\n",
    "        \"\"\"\n",
    "\n",
    "        job_config = bigquery.QueryJobConfig(\n",
    "            destination=dest,\n",
    "            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "            create_disposition=bigquery.CreateDisposition.CREATE_IF_NEEDED,\n",
    "        )\n",
    "\n",
    "        job = bq.query(sql, job_config=job_config)\n",
    "        job.result()  # Wait for completion\n",
    "\n",
    "        # Get stats\n",
    "        t = bq.get_table(dest)\n",
    "        stats = bq.query(f\"\"\"\n",
    "            SELECT\n",
    "                COUNT(*) as total,\n",
    "                COUNTIF(neighborhood IS NOT NULL) as with_neighborhood\n",
    "            FROM `{dest}`\n",
    "        \"\"\").to_dataframe().iloc[0]\n",
    "\n",
    "        result[\"rows_total\"]             = int(stats[\"total\"])\n",
    "        result[\"rows_with_neighborhood\"] = int(stats[\"with_neighborhood\"])\n",
    "        result[\"match_rate\"]             = round(stats[\"with_neighborhood\"] / stats[\"total\"] * 100, 1)\n",
    "        result[\"status\"]                 = \"SUCCESS\"\n",
    "\n",
    "        log.info(f\"  ✓ {result['rows_with_neighborhood']:,} / {result['rows_total']:,} rows matched ({result['match_rate']}%)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(f\"  ✗ Failed: {table_name} — {e}\")\n",
    "        result[\"status\"] = \"FAILED\"\n",
    "        result[\"error\"]  = str(e)\n",
    "\n",
    "    enrichment_log.append(result)\n",
    "\n",
    "log.info(\"\\n✓ Spatial enrichment complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e781f289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crime_incidents_legacy: ['neighborhood', '_ingested_at', '_source_url']\n",
      "vacant_building_notices: ['objectid', 'neighborhood', '_ingested_at', '_source_url']\n"
     ]
    }
   ],
   "source": [
    "for table_name in [\"crime_incidents_legacy\", \"vacant_building_notices\"]:\n",
    "    t = bq.get_table(f\"baltimore-analytics.raw_data.{table_name}\")\n",
    "    cols = [f.name for f in t.schema]\n",
    "    conflicts = [c for c in cols if c in [\"objectid\", \"neighborhood\", \"_ingested_at\", \"_source_url\"]]\n",
    "    print(f\"{table_name}: {conflicts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_s4",
   "metadata": {},
   "source": [
    "## 4. Copy Reference Tables\n",
    "Copy `real_property` and `neighborhood_boundaries` to `analytics` as-is — no spatial join needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell_reference",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:36:03,987 — INFO — Copying reference table: real_property\n",
      "2026-02-26 15:36:16,453 — INFO —   ✓ 238,496 rows → baltimore-analytics.analytics.real_property\n",
      "2026-02-26 15:36:16,454 — INFO — Copying reference table: neighborhood_boundaries\n",
      "2026-02-26 15:36:18,332 — INFO —   ✓ 279 rows → baltimore-analytics.analytics.neighborhood_boundaries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Reference tables copied.\n"
     ]
    }
   ],
   "source": [
    "for table_name in REFERENCE_TABLES:\n",
    "    log.info(f\"Copying reference table: {table_name}\")\n",
    "    try:\n",
    "        sql = f\"SELECT * FROM `{GCP_PROJECT}.{RAW_DATASET}.{table_name}`\"\n",
    "        dest = f\"{GCP_PROJECT}.{ANALYTICS_DATASET}.{table_name}\"\n",
    "\n",
    "        job_config = bigquery.QueryJobConfig(\n",
    "            destination=dest,\n",
    "            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "            create_disposition=bigquery.CreateDisposition.CREATE_IF_NEEDED,\n",
    "        )\n",
    "\n",
    "        bq.query(sql, job_config=job_config).result()\n",
    "        t = bq.get_table(dest)\n",
    "        log.info(f\"  ✓ {t.num_rows:,} rows → {dest}\")\n",
    "    except Exception as e:\n",
    "        log.error(f\"  ✗ Failed: {table_name} — {e}\")\n",
    "\n",
    "print(\"\\n✓ Reference tables copied.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_s5",
   "metadata": {},
   "source": [
    "## 5. Enrichment Audit Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell_audit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SPATIAL ENRICHMENT AUDIT REPORT\n",
      "Run at: 2026-02-26 20:36:28 UTC\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spenc\\AppData\\Local\\Temp\\ipykernel_26188\\1342754911.py:8: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  print(f\"Run at: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table</th>\n",
       "      <th>status</th>\n",
       "      <th>rows_total</th>\n",
       "      <th>rows_with_neighborhood</th>\n",
       "      <th>match_rate</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>crime_incidents_legacy</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>400 Duplicate column names in the result are not support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crime_incidents_current</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>400 Duplicate column names in the result are not support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bpd_arrests</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>400 Duplicate column names in the result are not support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vacant_building_notices</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>400 Duplicate column names in the result are not support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vacant_building_rehabs</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>400 Duplicate column names in the result are not support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>building_permits</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>400 Duplicate column names in the result are not support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>service_requests_311</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>400 Duplicate column names in the result are not support...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     table  status rows_total rows_with_neighborhood  \\\n",
       "0   crime_incidents_legacy  FAILED       None                   None   \n",
       "1  crime_incidents_current  FAILED       None                   None   \n",
       "2              bpd_arrests  FAILED       None                   None   \n",
       "3  vacant_building_notices  FAILED       None                   None   \n",
       "4   vacant_building_rehabs  FAILED       None                   None   \n",
       "5         building_permits  FAILED       None                   None   \n",
       "6     service_requests_311  FAILED       None                   None   \n",
       "\n",
       "  match_rate                                                        error  \n",
       "0       None  400 Duplicate column names in the result are not support...  \n",
       "1       None  400 Duplicate column names in the result are not support...  \n",
       "2       None  400 Duplicate column names in the result are not support...  \n",
       "3       None  400 Duplicate column names in the result are not support...  \n",
       "4       None  400 Duplicate column names in the result are not support...  \n",
       "5       None  400 Duplicate column names in the result are not support...  \n",
       "6       None  400 Duplicate column names in the result are not support...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚠ 7 table(s) failed — fix before proceeding to notebook 03.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "audit_df = pd.DataFrame(enrichment_log)\n",
    "pd.set_option(\"display.max_colwidth\", 60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SPATIAL ENRICHMENT AUDIT REPORT\")\n",
    "print(f\"Run at: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC\")\n",
    "print(\"=\"*70)\n",
    "display(audit_df[[\"table\", \"status\", \"rows_total\", \"rows_with_neighborhood\", \"match_rate\", \"error\"]])\n",
    "\n",
    "failed = audit_df[audit_df[\"status\"] == \"FAILED\"]\n",
    "if len(failed) > 0:\n",
    "    print(f\"\\n⚠ {len(failed)} table(s) failed — fix before proceeding to notebook 03.\")\n",
    "else:\n",
    "    print(f\"\\n✓ All {len(audit_df)} tables enriched successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_s6",
   "metadata": {},
   "source": [
    "## 6. Verify analytics Dataset\n",
    "Confirm all 9 tables landed in `analytics` with expected row counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"analytics dataset contents:\\n\")\n",
    "all_tables = POINT_TABLES + REFERENCE_TABLES\n",
    "for table_name in all_tables:\n",
    "    try:\n",
    "        t = bq.get_table(f\"{GCP_PROJECT}.{ANALYTICS_DATASET}.{table_name}\")\n",
    "        print(f\"✅ {table_name}: {t.num_rows:,} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {table_name}: missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29488ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ real_property refreshed in analytics\n"
     ]
    }
   ],
   "source": [
    "bq.query(\"\"\"\n",
    "    SELECT * FROM `baltimore-analytics.raw_data.real_property`\n",
    "\"\"\", job_config=bigquery.QueryJobConfig(\n",
    "    destination=\"baltimore-analytics.analytics.real_property\",\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "    create_disposition=bigquery.CreateDisposition.CREATE_IF_NEEDED\n",
    ")).result()\n",
    "print(\"✓ real_property refreshed in analytics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_s7",
   "metadata": {},
   "source": [
    "## 7. Spot Check — Neighborhood Match Quality\n",
    "Sample neighborhood distribution for a few tables to sanity check the spatial join results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_spotcheck",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 neighborhoods by crime (legacy)\n",
    "print(\"Top 10 neighborhoods by crime incidents (legacy):\")\n",
    "display(bq.query(\"\"\"\n",
    "    SELECT neighborhood, COUNT(*) as incidents\n",
    "    FROM `baltimore-analytics.analytics.crime_incidents_legacy`\n",
    "    WHERE neighborhood IS NOT NULL\n",
    "    GROUP BY neighborhood\n",
    "    ORDER BY incidents DESC\n",
    "    LIMIT 10\n",
    "\"\"\").to_dataframe())\n",
    "\n",
    "# Top 10 neighborhoods by 311 requests\n",
    "print(\"\\nTop 10 neighborhoods by 311 service requests:\")\n",
    "display(bq.query(\"\"\"\n",
    "    SELECT neighborhood, COUNT(*) as requests\n",
    "    FROM `baltimore-analytics.analytics.service_requests_311`\n",
    "    WHERE neighborhood IS NOT NULL\n",
    "    GROUP BY neighborhood\n",
    "    ORDER BY requests DESC\n",
    "    LIMIT 10\n",
    "\"\"\").to_dataframe())\n",
    "\n",
    "# Top 10 neighborhoods by vacant building notices\n",
    "print(\"\\nTop 10 neighborhoods by vacant building notices:\")\n",
    "display(bq.query(\"\"\"\n",
    "    SELECT neighborhood, COUNT(*) as notices\n",
    "    FROM `baltimore-analytics.analytics.vacant_building_notices`\n",
    "    WHERE neighborhood IS NOT NULL\n",
    "    GROUP BY neighborhood\n",
    "    ORDER BY notices DESC\n",
    "    LIMIT 10\n",
    "\"\"\").to_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_next",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "Once all tables show `SUCCESS` and spot checks look reasonable:\n",
    "\n",
    "1. **`03_feature_engineering.ipynb`** — Aggregate all tables to NSA level, build Neighborhood Vitality Index feature matrix\n",
    "2. **`04_clustering.ipynb`** — K-means segmentation of neighborhoods into cohorts\n",
    "3. **`05_looker_studio.md`** — Dashboard connection guide\n",
    "\n",
    "**If match rates are lower than expected:**  \n",
    "Check if neighborhoods at city boundaries have points that fall slightly outside polygon edges — may need `ST_DWITHIN` with a small buffer as fallback.\n",
    "\n",
    "**Re-running this notebook:**  \n",
    "All queries use `WRITE_TRUNCATE` — safe to re-run at any time without duplicating data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
