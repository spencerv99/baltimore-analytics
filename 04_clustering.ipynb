{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baltimore Analytics — Neighborhood Clustering\n",
    "**Project:** `baltimore-analytics`  \n",
    "**Input:** `analytics.neighborhood_features`  \n",
    "**Output:** `analytics.neighborhood_clusters` + `neighborhood_clusters.csv`  \n",
    "**Author:** Spencer  \n",
    "\n",
    "---\n",
    "### What This Notebook Does\n",
    "Uses K-means clustering to segment Baltimore's 279 neighborhoods into cohorts based on the feature matrix built in notebook 03. Cluster labels are written back to BigQuery for use in Looker Studio.\n",
    "\n",
    "### Approach\n",
    "1. Load feature matrix from BigQuery\n",
    "2. Select and scale clustering features\n",
    "3. Use elbow method + silhouette scores to determine optimal K\n",
    "4. Fit final K-means model\n",
    "5. Profile and label each cluster\n",
    "6. Write results to BigQuery + CSV\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run once\n",
    "# %pip install scikit-learn matplotlib seaborn google-cloud-bigquery pandas pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s — %(levelname)s — %(message)s')\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "GCP_PROJECT       = \"baltimore-analytics\"\n",
    "ANALYTICS_DATASET = \"analytics\"\n",
    "GCP_REGION        = \"us-east1\"\n",
    "CREDENTIALS_PATH  = \"service_account.json\"\n",
    "\n",
    "# Minimum population to include in clustering — excludes parks/industrial areas\n",
    "MIN_POPULATION = 100\n",
    "\n",
    "# K range to evaluate\n",
    "K_MIN = 2\n",
    "K_MAX = 10\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"✓ Config loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Client & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    CREDENTIALS_PATH,\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    ")\n",
    "bq = bigquery.Client(project=GCP_PROJECT, credentials=credentials)\n",
    "\n",
    "# Load feature matrix\n",
    "df = bq.query(f\"\"\"\n",
    "    SELECT * FROM `{GCP_PROJECT}.{ANALYTICS_DATASET}.neighborhood_features`\n",
    "    ORDER BY neighborhood\n",
    "\"\"\").to_dataframe()\n",
    "\n",
    "print(f\"✓ Loaded {len(df)} neighborhoods × {len(df.columns)} features\")\n",
    "\n",
    "# Neighborhoods to exclude from residential clustering:\n",
    "# - Industrial/entertainment areas with artificially inflated crime per capita\n",
    "#   due to near-zero residential population\n",
    "# - Institutional campuses (universities, hospitals) with atypical property patterns\n",
    "# These are analyzed separately in Cell 9.\n",
    "EXCLUDE_NEIGHBORHOODS = [\n",
    "    # High-crime-per-capita industrial/entertainment (low residential pop)\n",
    "    \"Canton Industrial Area\", \"Fairfield Area\", \"Inner Harbor\",\n",
    "    \"Pulaski Industrial Area\", \"Stadium/Entertainment Area\",\n",
    "    # Institutional campuses\n",
    "    \"Dunbar-Broadway\", \"Hopkins Bayview\", \"Johns Hopkins Homewood\",\n",
    "    \"Morgan State University\", \"Penn-Fallsway\", \"Perkins\",\n",
    "    \"University of Maryland\", \"Harbor Point\",\n",
    "]\n",
    "\n",
    "df_full = df.copy()\n",
    "df = df[\n",
    "    (df[\"population\"] >= MIN_POPULATION) &\n",
    "    (~df[\"neighborhood\"].isin(EXCLUDE_NEIGHBORHOODS))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(f\"✓ {len(df)} neighborhoods after filtering\")\n",
    "print(f\"  Low population excluded: {df_full[df_full['population'] < MIN_POPULATION]['neighborhood'].tolist()}\")\n",
    "print(f\"  Institutional/industrial excluded: {EXCLUDE_NEIGHBORHOODS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select & Prepare Clustering Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mstats\n",
    "\n",
    "# Per-capita and rate features preferred over raw counts to avoid population bias\n",
    "CLUSTERING_FEATURES = [\n",
    "    # Demographics\n",
    "    \"population\",\n",
    "    \"med_age\",\n",
    "    \"housing_vacancy_rate\",\n",
    "    \"owner_occupancy_rate\",\n",
    "    # Safety\n",
    "    \"crime_per_capita\",\n",
    "    \"arrest_per_capita\",\n",
    "    \"crime_yoy_change\",\n",
    "    # Housing / Vacancy\n",
    "    \"vacant_notices_per_unit\",\n",
    "    \"rehab_to_vacant_ratio\",\n",
    "    # Investment\n",
    "    \"permit_value_per_capita\",\n",
    "    \"permit_recent_5yr\",\n",
    "    # Quality of life\n",
    "    \"requests_311_per_capita\",\n",
    "    \"requests_311_blight\",\n",
    "    # Property values\n",
    "    \"median_assessed_value\",\n",
    "    \"avg_assessed_value\",\n",
    "]\n",
    "\n",
    "available = [f for f in CLUSTERING_FEATURES if f in df.columns]\n",
    "missing   = [f for f in CLUSTERING_FEATURES if f not in df.columns]\n",
    "if missing:\n",
    "    print(f\"⚠ Missing features (skipped): {missing}\")\n",
    "\n",
    "X_raw = df[available].copy()\n",
    "print(f\"✓ {len(available)} features selected for clustering\")\n",
    "\n",
    "# Impute missing values with column median\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_imputed = imputer.fit_transform(X_raw)\n",
    "null_counts = X_raw.isnull().sum()\n",
    "if null_counts.sum() > 0:\n",
    "    print(f\"  Imputed nulls:\")\n",
    "    print(null_counts[null_counts > 0])\n",
    "\n",
    "# Winsorize at 1st/99th percentile to prevent extreme outliers\n",
    "# from dominating their own clusters\n",
    "X_winsorized = mstats.winsorize(X_imputed, limits=[0.01, 0.01], axis=0)\n",
    "\n",
    "# Standardize — K-means is distance-based, scale matters\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_winsorized)\n",
    "\n",
    "print(f\"✓ Features winsorized and scaled. Shape: {X_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Elbow Method + Silhouette Scores\n",
    "Evaluate K from 2 to 10. Look for the elbow in inertia and peak in silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range     = range(K_MIN, K_MAX + 1)\n",
    "inertias    = []\n",
    "silhouettes = []\n",
    "\n",
    "print(f\"Evaluating K = {K_MIN} to {K_MAX}...\\n\")\n",
    "print(f\"{'K':>4}  {'Inertia':>12}  {'Silhouette':>12}\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "for k in k_range:\n",
    "    km = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
    "    labels = km.fit_predict(X_scaled)\n",
    "    inertias.append(km.inertia_)\n",
    "    sil = silhouette_score(X_scaled, labels)\n",
    "    silhouettes.append(sil)\n",
    "    print(f\"{k:>4}  {km.inertia_:>12.1f}  {sil:>12.4f}\")\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(list(k_range), inertias, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('K (Number of Clusters)', fontsize=12)\n",
    "ax1.set_ylabel('Inertia (Within-cluster Sum of Squares)', fontsize=12)\n",
    "ax1.set_title('Elbow Method', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(list(k_range), silhouettes, 'ro-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('K (Number of Clusters)', fontsize=12)\n",
    "ax2.set_ylabel('Silhouette Score', fontsize=12)\n",
    "ax2.set_title('Silhouette Score by K', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "best_k = list(k_range)[silhouettes.index(max(silhouettes))]\n",
    "ax2.axvline(x=best_k, color='green', linestyle='--', label=f'Best K = {best_k}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('elbow_silhouette.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\n✓ Best K by silhouette: {best_k}\")\n",
    "print(f\"  Review the elbow plot and override K_FINAL below if needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Top-Level Split — K=2\n",
    "Fit the primary K=2 split that divides neighborhoods into Stable and Distressed groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_TOP = 2\n",
    "km_top = KMeans(n_clusters=K_TOP, random_state=RANDOM_STATE, n_init=20)\n",
    "df[\"top_cluster\"] = km_top.fit_predict(X_scaled)\n",
    "\n",
    "print(\"Top-level cluster sizes:\")\n",
    "print(df[\"top_cluster\"].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nCluster profiles:\")\n",
    "print(df.groupby(\"top_cluster\")[[\"crime_per_capita\", \"vacant_notices_per_unit\",\n",
    "                                   \"median_assessed_value\", \"housing_vacancy_rate\",\n",
    "                                   \"permit_value_per_capita\"]].mean().round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sub-Clustering\n",
    "Sub-cluster each top-level group independently. Silhouette determines optimal K (range 2-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k(X, k_range=range(2, 6), random_state=42):\n",
    "    \"\"\"Find optimal K using silhouette score.\"\"\"\n",
    "    best_k, best_score = 2, -1\n",
    "    scores = {}\n",
    "    for k in k_range:\n",
    "        if len(X) <= k:\n",
    "            break\n",
    "        km = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n",
    "        labels = km.fit_predict(X)\n",
    "        score = silhouette_score(X, labels)\n",
    "        scores[k] = round(score, 4)\n",
    "        if score > best_score:\n",
    "            best_k, best_score = k, score\n",
    "    return best_k, best_score, scores\n",
    "\n",
    "\n",
    "sub_cluster_col = \"sub_cluster\"\n",
    "df[sub_cluster_col] = -1\n",
    "sub_results = {}\n",
    "\n",
    "avg_crime = df[\"crime_per_capita\"].mean()\n",
    "\n",
    "for top in sorted(df[\"top_cluster\"].unique()):\n",
    "    mask = df[\"top_cluster\"] == top\n",
    "    X_sub = X_scaled[mask.values]\n",
    "    grp_crime = df.loc[mask, \"crime_per_capita\"].mean()\n",
    "    label = \"Stable\" if grp_crime < avg_crime else \"Distressed\"\n",
    "\n",
    "    print(f\"\\nTop cluster {top} ({label}) - {mask.sum()} neighborhoods\")\n",
    "    best_k, best_score, scores = find_best_k(X_sub)\n",
    "    print(f\"  Silhouette scores: {scores}\")\n",
    "    print(f\"  Best K: {best_k} (score: {best_score:.4f})\")\n",
    "\n",
    "    km_sub = KMeans(n_clusters=best_k, random_state=RANDOM_STATE, n_init=20)\n",
    "    sub_labels = km_sub.fit_predict(X_sub)\n",
    "\n",
    "    # Offset sub-cluster IDs so they dont overlap across top clusters\n",
    "    offset = top * 10\n",
    "    df.loc[mask, sub_cluster_col] = sub_labels + offset\n",
    "    sub_results[top] = {\"label\": label, \"best_k\": best_k, \"best_score\": best_score}\n",
    "\n",
    "print(\"\\nFinal sub-cluster sizes:\")\n",
    "print(df[sub_cluster_col].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PCA Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "pca_df = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"])\n",
    "pca_df[\"top_cluster\"]  = df[\"top_cluster\"].values\n",
    "pca_df[\"sub_cluster\"]  = df[sub_cluster_col].values\n",
    "pca_df[\"neighborhood\"] = df[\"neighborhood\"].values\n",
    "\n",
    "unique_subs = sorted(pca_df[\"sub_cluster\"].unique())\n",
    "colors = cm.nipy_spectral(np.linspace(0, 1, len(unique_subs)))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "for i in sorted(pca_df[\"top_cluster\"].unique()):\n",
    "    mask = pca_df[\"top_cluster\"] == i\n",
    "    label = sub_results[i][\"label\"]\n",
    "    ax1.scatter(pca_df.loc[mask, \"PC1\"], pca_df.loc[mask, \"PC2\"],\n",
    "               label=f\"Cluster {i} ({label})\", s=60, alpha=0.8)\n",
    "ax1.set_title(\"Top-Level K=2 Split\", fontsize=13)\n",
    "ax1.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "ax1.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "for i, sub in enumerate(unique_subs):\n",
    "    mask = pca_df[\"sub_cluster\"] == sub\n",
    "    ax2.scatter(pca_df.loc[mask, \"PC1\"], pca_df.loc[mask, \"PC2\"],\n",
    "               c=[colors[i]], label=f\"Sub-cluster {sub}\", s=60, alpha=0.8)\n",
    "ax2.set_title(\"Hierarchical Sub-Clusters\", fontsize=13)\n",
    "ax2.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "ax2.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "ax2.legend(fontsize=8)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Baltimore Neighborhood Clustering - Hierarchical K-Means\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pca_clusters.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cluster Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = df.groupby(sub_cluster_col)[available].mean().round(3).T\n",
    "print(\"Sub-cluster feature profiles (mean values):\")\n",
    "display(profile)\n",
    "\n",
    "profile_norm = profile.apply(lambda x: (x - x.mean()) / x.std() if x.std() > 0 else x * 0, axis=1).fillna(0)\n",
    "n_subs = len(df[sub_cluster_col].unique())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(max(8, n_subs * 2), len(available) * 0.4 + 2))\n",
    "sns.heatmap(profile_norm, annot=False, cmap=\"RdYlGn_r\", center=0,\n",
    "            linewidths=0.5, ax=ax, cbar_kws={\"label\": \"Z-score (red=high, green=low)\"})\n",
    "ax.set_title(\"Sub-Cluster Feature Profiles\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cluster_heatmap.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "for sub in sorted(df[sub_cluster_col].unique()):\n",
    "    hoods = df[df[sub_cluster_col] == sub][\"neighborhood\"].sort_values().tolist()\n",
    "    top = sub // 10\n",
    "    print(f\"\\nSub-cluster {sub} ({sub_results[top]['label']}) - {len(hoods)} neighborhoods:\")\n",
    "    print(\", \".join(hoods))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Excluded Neighborhood Analysis\n",
    "Separately cluster low-population (parks) vs institutional/industrial neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_NEIGHBORHOODS = [\n",
    "    \"Canton Industrial Area\", \"Fairfield Area\", \"Inner Harbor\",\n",
    "    \"Pulaski Industrial Area\", \"Stadium/Entertainment Area\",\n",
    "    \"Dunbar-Broadway\", \"Hopkins Bayview\", \"Johns Hopkins Homewood\",\n",
    "    \"Morgan State University\", \"Penn-Fallsway\", \"Perkins\",\n",
    "    \"University of Maryland\",\n",
    "]\n",
    "\n",
    "df_excluded = df_full[\n",
    "    (df_full[\"population\"] < MIN_POPULATION) |\n",
    "    (df_full[\"neighborhood\"].isin(EXCLUDE_NEIGHBORHOODS))\n",
    "].copy().reset_index(drop=True)\n",
    "\n",
    "df_excluded[\"excluded_type\"] = df_excluded[\"neighborhood\"].apply(\n",
    "    lambda n: \"Institutional/Industrial\" if n in EXCLUDE_NEIGHBORHOODS else \"Low Population (Park/Open Space)\"\n",
    ")\n",
    "\n",
    "print(f\"Total excluded neighborhoods: {len(df_excluded)}\")\n",
    "print(df_excluded.groupby(\"excluded_type\")[\"neighborhood\"].count())\n",
    "\n",
    "df_excluded[\"excluded_cluster\"] = \"Unclustered\"\n",
    "\n",
    "for exc_type in df_excluded[\"excluded_type\"].unique():\n",
    "    mask = df_excluded[\"excluded_type\"] == exc_type\n",
    "    subset = df_excluded[mask].copy()\n",
    "    avail = [f for f in available if f in subset.columns]\n",
    "\n",
    "    X_exc = SimpleImputer(strategy=\"median\").fit_transform(subset[avail])\n",
    "    X_exc = StandardScaler().fit_transform(X_exc)\n",
    "\n",
    "    if len(subset) < 4:\n",
    "        df_excluded.loc[mask, \"excluded_cluster\"] = exc_type + \" - Too few to cluster\"\n",
    "        print(f\"\\n{exc_type}: only {len(subset)} neighborhoods, skipping\")\n",
    "        continue\n",
    "\n",
    "    best_k, best_score, scores = find_best_k(X_exc, k_range=range(2, min(5, len(subset))))\n",
    "    print(f\"\\n{exc_type} ({len(subset)} neighborhoods) - Best K: {best_k} (silhouette: {best_score:.4f})\")\n",
    "\n",
    "    km_exc = KMeans(n_clusters=best_k, random_state=RANDOM_STATE, n_init=10)\n",
    "    labels = km_exc.fit_predict(X_exc)\n",
    "    df_excluded.loc[mask, \"excluded_cluster\"] = [f\"{exc_type} - Group {l}\" for l in labels]\n",
    "\n",
    "print(\"\\nExcluded cluster assignments:\")\n",
    "display(df_excluded[[\"neighborhood\", \"excluded_type\", \"excluded_cluster\",\n",
    "                      \"crime_per_capita\", \"median_assessed_value\"]].sort_values(\"excluded_cluster\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Label Clusters\n",
    "Review the profiles in Cell 8 and fill in `SUB_CLUSTER_LABELS`. Sub-clusters in the Stable group use IDs 0x; Distressed group uses 1x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── EDIT THESE after reviewing Cell 8 profiles ───────────────────────────\n",
    "# Sub-cluster IDs: stable group uses 0x, distressed group uses 1x\n",
    "\n",
    "SUB_CLUSTER_LABELS = {\n",
    "    0:  \"Stable — Working Class Rowhouse\",\n",
    "    1:  \"Stable — Mixed/Transitional\",\n",
    "    2:  \"Stable — Gentrifying/Hip\",\n",
    "    3:  \"Stable — Quiet Residential\",\n",
    "    4:  \"Stable — Affluent North Baltimore\",\n",
    "    10: \"Distressed — Core Disinvestment\",\n",
    "    11: \"Distressed — Downtown/Commercial Fringe\",\n",
    "}\n",
    "unique_subs = sorted(df[sub_cluster_col].unique())\n",
    "SUB_CLUSTER_LABELS = {s: f\"Cluster {s}\" for s in unique_subs}\n",
    "\n",
    "df[\"cluster_label\"] = df[sub_cluster_col].map(SUB_CLUSTER_LABELS)\n",
    "df[\"top_cluster_label\"] = df[\"top_cluster\"].map(\n",
    "    {t: sub_results[t][\"label\"] for t in sub_results}\n",
    ")\n",
    "\n",
    "print(\"Final cluster assignments:\")\n",
    "print(df.groupby([\"top_cluster_label\", \"cluster_label\"])[\"neighborhood\"]\n",
    "        .count().reset_index().rename(columns={\"neighborhood\": \"count\"}).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Write Results to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residential neighborhoods\n",
    "residential_out = df[[\"neighborhood\", \"top_cluster\", \"top_cluster_label\",\n",
    "                       sub_cluster_col, \"cluster_label\"]].copy()\n",
    "residential_out[\"excluded_type\"]    = \"Residential\"\n",
    "residential_out[\"excluded_cluster\"] = None\n",
    "\n",
    "# Excluded neighborhoods\n",
    "excluded_out = df_excluded[[\"neighborhood\", \"excluded_type\", \"excluded_cluster\"]].copy()\n",
    "excluded_out[\"top_cluster\"]       = -1\n",
    "excluded_out[\"top_cluster_label\"] = \"Excluded\"\n",
    "excluded_out[sub_cluster_col]     = -1\n",
    "excluded_out[\"cluster_label\"]     = excluded_out[\"excluded_cluster\"]\n",
    "\n",
    "# Combine and merge back to full feature matrix\n",
    "combined = pd.concat([residential_out, excluded_out], ignore_index=True)\n",
    "output_df = df_full.merge(combined, on=\"neighborhood\", how=\"left\")\n",
    "output_df[\"_clustered_at\"] = pd.Timestamp.utcnow().strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "\n",
    "dest = f\"{GCP_PROJECT}.{ANALYTICS_DATASET}.neighborhood_clusters\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "    autodetect=True\n",
    ")\n",
    "bq.load_table_from_dataframe(output_df, dest, job_config=job_config).result()\n",
    "t = bq.get_table(dest)\n",
    "print(f\"✓ {t.num_rows:,} rows → {dest}\")\n",
    "\n",
    "output_df.to_csv(\"neighborhood_clusters.csv\", index=False)\n",
    "df_excluded.to_csv(\"excluded_neighborhood_analysis.csv\", index=False)\n",
    "print(\"✓ Saved neighborhood_clusters.csv\")\n",
    "print(\"✓ Saved excluded_neighborhood_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CLUSTERING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Residential neighborhoods clustered: {len(df)}\")\n",
    "print(f\"Excluded neighborhoods analyzed:     {len(df_excluded)}\")\n",
    "print(f\"Total in output table:               {len(output_df)}\")\n",
    "print()\n",
    "\n",
    "summary = df.groupby([\"top_cluster_label\", \"cluster_label\"]).agg(\n",
    "    neighborhoods        = (\"neighborhood\", \"count\"),\n",
    "    avg_crime_per_capita = (\"crime_per_capita\", \"mean\"),\n",
    "    avg_vacancy_rate     = (\"housing_vacancy_rate\", \"mean\"),\n",
    "    avg_assessed_value   = (\"median_assessed_value\", \"mean\"),\n",
    "    avg_permit_value     = (\"permit_value_per_capita\", \"mean\"),\n",
    ").round(3).reset_index()\n",
    "display(summary)\n",
    "\n",
    "print(f\"\"\"\n",
    "Output files:\n",
    "  BigQuery: {GCP_PROJECT}.{ANALYTICS_DATASET}.neighborhood_clusters\n",
    "  CSV:      neighborhood_clusters.csv\n",
    "  CSV:      excluded_neighborhood_analysis.csv\n",
    "  Charts:   pca_clusters.png, cluster_heatmap.png\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "1. **`05_looker_studio.md`** — Connect `analytics.neighborhood_clusters` to Looker Studio\n",
    "\n",
    "**Key fields for Looker Studio:**\n",
    "- `neighborhood` — join key\n",
    "- `top_cluster_label` — Stable / Distressed (primary filter)\n",
    "- `cluster_label` — sub-cluster cohort label\n",
    "- Join to `neighborhood_boundaries` for polygon map layer\n",
    "\n",
    "**To re-label clusters:**\n",
    "Edit `SUB_CLUSTER_LABELS` in Cell 10 and re-run Cells 10-12.\n",
    "\n",
    "**To change sub-cluster K:**\n",
    "The `find_best_k()` function auto-selects — override by setting a fixed value in Cell 6."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
